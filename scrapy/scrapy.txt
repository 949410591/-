tutorial
  创建scrapy项目
    scrapy startproject 【project_name]

    spider
      start_requests
        url 列表或者生成器都可以使用

      parse(self, response)
        response 确定是那个页面的内容
        parse 方法会自动找到新的urls 然后创建相应的requests

  运行爬虫
    scrapy crawl [spder_name]

  scrapy的运行机制
    scrapy.Request 返回 response(instantiate) 然后把response 作为参数返回给parse

  处理数据
    css/xpath
      看其它文本
    scrapy shell [url]
      注意url必须用引号包括，不然不会运行
    response.css('title::text')
      ::text 是指此标签的内容
    response.css('title::text').getall()
      getall() 获得所有结果
    response.css('title::text')[0].get()
      get() 是获得一个，[num]是指第几个
    response.css('title::text').re(r'Quotes.*')
      允许在选择器之上允许运行正则表达式

  蜘蛛中提取数据
    for  quote in response.xpath('//div[@class="quote"]'):
      yield {
          'text':quote.xpath('./span/text()').get(),
          'author':quote.xpath('./span/small/text()').get(),
          'tags':quote.xpath('./div/a/text()').getall()
      }
      返回数据就是使用yield把爬取的数据一个个返回给引擎

  跟随链接
    1
      nextpage = response.xpath('...').get()
      if nextpage != None:
        nextpage = response.urljion(nextpage)
        yield scrapy.Request(nextpae, callback=self.parse)

    2
      nextpage = response.xpath('xxx').get()
      if nextpage != None:
        yield response.follow(nextpage, callback=self.parse)

    3
      a = response.xpath('..').get()
      if a :
        yield response.follow(a, callback=self.parse)

  其它例子
    start_urls = ['http://quotes.toscrape.com/']
    def parse(self, response):
    # follow links to author pages
      for href in response.css('.author + a::attr(href)'):
        yield response.follow(href, self.parse_author)
    # follow pagination links
      for href in response.css('li.next a::attr(href)'):
        yield response.follow(href, self.parse)
    def parse_author(self, response):
      def extract_with_css(query):
        return response.css(query).get(default='').strip()
      yield {
        'name': extract_with_css('h3.author-title::text'),
        'birthdate': extract_with_css('.author-born-date::text'),
        'bio':extract_with_css('.author-description::text'),
      }

    再原始parse进行跟随，然后有些链接把callback设置为parse_author
	
命令行工具
	配置文件
	多个项目共享根目录
	使用scrapy工具
		不带参数scrapy
			获很多基础信息
			如果当前工作目录不在项目目录里
				Scrapy X.Y - no active project
				Usage:
				scrapy <command> [options] [args]
				Available commands:
				crawl Run a spider
				fetch Fetch a URL using the Scrapy downloader
				[...]
				主要显示一些命令的使用
			如果当前工作目录在项目目录里
				Scrapy X.Y - project: myproject
				Usage:
				scrapy <command> [options] [args]
				[...]
				
			不同之处就是第一行	
		
		创建项目
			scrapy startproject myproject [project_dir]
				如果没有[project_dir]这个参数会创建这样的项目
					tutorial/
					├── scrapy.cfg
					└── tutorial
						├── __init__.py
						├── items.py
						├── middlewares.py
						├── pipelines.py
						├── __pycache__
						├── settings.py
						└── spiders
							├── __init__.py
							└── __pycache__
				如果有
					test
					├── scrapy.cfg
					└── tutoril
						├── __init__.py
						├── items.py
						├── middlewares.py
						├── pipelines.py
						├── __pycache__
						├── settings.py
						└── spiders
							├── __init__.py
							└── __pycache__
				就是项目根目录的名字差别
		
		控制项目
	可获得的工具命令
		获取命令帮助
			scrapy <command> -h
		获取可用命令
			scrapy -h
		
		两种命令
			一种是需要在项目文件夹运行的，另一种不需要
			全局命令可能在项目文件夹运行时会和全局运行时有些不同
			
		Global commands:
			• startproject
				scrapy startproject myproject [project_dir]
				如果没有[project_dir]这个参数会创建这样的项目
					tutorial/
					├── scrapy.cfg
					└── tutorial
						├── __init__.py
						├── items.py
						├── middlewares.py
						├── pipelines.py
						├── __pycache__
						├── settings.py
						└── spiders
							├── __init__.py
							└── __pycache__
				如果有
					test
					├── scrapy.cfg
					└── tutoril
						├── __init__.py
						├── items.py
						├── middlewares.py
						├── pipelines.py
						├── __pycache__
						├── settings.py
						└── spiders
							├── __init__.py
							└── __pycache__
				就是项目根目录的名字差别
		
			• genspider
				scrapy genspider [-t template] <name> <domain>
				查询可用模板
					scrapy genspider -l 
				
				需要创建模板爬虫记得加-t
					scrapy genspider -t crawl scrapyorg scrapy.org
				
				如果不想使用这个命令也可以自己在爬虫源文件夹创建文件
					
			• settings
				scrapy settings [options]
				可以获取项目设置的值
				选项看-h更详细
		
			• runspider
				scrapy runspider <spider_file.py>
				可以运行项目之外的单独的爬虫
			• shell
				scrapy shell [url]
					支持本地文件，可以是相对路径也可以是绝对路径
					• --spider=SPIDER: bypass spider autodetection and force use of specific spider
					• -c code: evaluate the code in the shell, print the result and exit
						(scrapy_env) kk@kk-VirtualBox:~/test$ scrapy shell --nolog 'https://www.baidu.com' -c '(response.status,response.url)'
						(200, 'https://www.baidu.com')

					• --no-redirect: do not follow HTTP 3xx redirects (default is to follow them); this only affects the URL
					you may pass as argument on the command line; once you are inside the shell, fetch(url) will still follow
					HTTP redirects by default.
		
				
			• fetch
				scrapy fetch <url>
					Supported options:
						• --spider=SPIDER: bypass spider autodetection and force use of specific spider
							注意在使用特定爬虫时，如果这个爬虫有user_agent这个属性会覆盖当前的user agent
						• --headers: print the response’s HTTP headers instead of the response’s body
						• --no-redirect: do not follow HTTP 3xx redirects (default is to follow them)
	
			• view
				scrapy view <url>
				确保爬虫爬出来的网页内容是你期望看到的
			• version
		Project-only commands:
			• crawl
				scrapy crawl <spider>
			• check	
			• list
				在当前文件夹下列出可找到的爬虫
				scrapy list
			• edit
			• parse
				以爬虫的方式处理url
				scrapy parse <url> [options]
				选项
					• --spider=SPIDER: bypass spider autodetection and force use of specific spider
					• --a NAME=VALUE: set spider argument (may be repeated)
					• --callback or -c: spider method to use as callback for parsing the response
					• --meta or -m: additional request meta that will be passed to the callback request. This must be a valid json
					string. Example: –meta=’{“foo” : “bar”}’
					• --cbkwargs: additional keyword arguments that will be passed to the callback. This must be a valid json
					string. Example: –cbkwargs=’{“foo” : “bar”}’
					• --pipelines: process items through pipelines
					• --rules or -r: use CrawlSpider rules to discover the callback (i.e. spider method) to use for parsing the
					response
					• --noitems: don’t show scraped items
					• --nolinks: don’t show extracted links
					• --nocolour: avoid using pygments to colorize the output
					• --depth or -d: depth level for which the requests should be followed recursively (default: 1)
					• --verbose or -v: display information for each depth level
			• bench
				测试用的具体看文档
	定制项目命令
	
	
	
	
	